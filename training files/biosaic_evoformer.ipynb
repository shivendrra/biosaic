{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP15LYUyupXwXl5245EoRlx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/biosaic/blob/main/training%20files/biosaic_evoformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wStocCMwaAca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "FDVFR1jgZ9W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "  DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  A            = 4        # DNA alphabet\n",
        "  C            = 21       # 21 letter for amino acid & 4 for dna\n",
        "  d_msa        = 128\n",
        "  d_pair       = 64\n",
        "  n_heads      = 8\n",
        "  n_blocks     = 4"
      ],
      "metadata": {
        "id": "eB4BWfncZ8KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ei-8IrjZ1tV"
      },
      "outputs": [],
      "source": [
        "class RowAttention(nn.Module):\n",
        "  def __init__(self, d_msa, n_heads):\n",
        "    super().__init__()\n",
        "    self.attn = nn.MultiheadAttention(d_msa, n_heads, batch_first=True)\n",
        "  def forward(self, msa):  # msa: (B, N, L, d_msa)\n",
        "    B, N, L, D = msa.shape\n",
        "    x = msa.view(B*L, N, D)  # treat each position across sequences as a sequence\n",
        "    out, _ = self.attn(x, x, x)\n",
        "    return out.view(B, N, L, D)\n",
        "\n",
        "class ColAttention(nn.Module):\n",
        "  def __init__(self, d_msa, n_heads):\n",
        "    super().__init__()\n",
        "    self.attn = nn.MultiheadAttention(d_msa, n_heads, batch_first=True)\n",
        "  def forward(self, msa):\n",
        "    B, N, L, D = msa.shape\n",
        "    x = msa.permute(0,2,1,3).reshape(B* N, L, D)  # each sequence across positions\n",
        "    out, _ = self.attn(x, x, x)\n",
        "    return out.view(B, L, N, D).permute(0,2,1,3)\n",
        "\n",
        "class TriMulUpdate(nn.Module):\n",
        "  def __init__(self, d_pair):\n",
        "    super().__init__()\n",
        "    self.linear_a = nn.Linear(d_pair, d_pair)\n",
        "    self.linear_b = nn.Linear(d_pair, d_pair)\n",
        "  def forward(self, pair):\n",
        "    # pair: (B, L, L, d_pair)\n",
        "    left = self.linear_a(pair)    # (B,L,L,d)\n",
        "    right= self.linear_b(pair)    # (B,L,L,d)\n",
        "    # outer product along one axis\n",
        "    # simplistic: new_pair[i,j] += sum_k left[i,k] * right[k,j]\n",
        "    return pair + torch.einsum(\"bikd,bkjd->bijd\", left, right)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, d_msa, d_pair, n_heads):\n",
        "    super().__init__()\n",
        "    self.row_attn = RowAttention(d_msa, n_heads)\n",
        "    self.col_attn = ColAttention(d_msa, n_heads)\n",
        "    self.tri_mul = TriMulUpdate(d_pair)\n",
        "    # plus feed‑forwards, layernorms, gating, etc.\n",
        "\n",
        "  def forward(self, msa, pair):\n",
        "    msa = msa + self.row_attn(msa)\n",
        "    msa = msa + self.col_attn(msa)\n",
        "    pair= pair + self.tri_mul(pair)\n",
        "    return msa, pair\n",
        "\n",
        "class Evoformer(nn.Module):\n",
        "  def __init__(self, params: ModelConfig):\n",
        "    \"\"\"\n",
        "      A: alphabet size (e.g. 4 for DNA, 21 for protein)\n",
        "      C: number of initial pair features\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.embed_msa  = nn.Linear(ModelConfig.A, ModelConfig.d_msa)\n",
        "    self.embed_pair = nn.Linear(ModelConfig.C, ModelConfig.d_pair)\n",
        "    self.blocks     = nn.ModuleList([\n",
        "      Block(ModelConfig.d_msa, ModelConfig.d_pair, ModelConfig.n_heads)\n",
        "      for _ in range(ModelConfig.n_blocks)\n",
        "    ])\n",
        "    # for masked token prediction\n",
        "    self.msa_out = nn.Linear(ModelConfig.d_msa, ModelConfig.A)\n",
        "  def forward(self, msa, pair):\n",
        "    # msa: (B, N, L, A); pair: (B, L, L, C)\n",
        "    msa  = self.embed_msa(msa)\n",
        "    pair = self.embed_pair(pair)\n",
        "    for blk in self.blocks:\n",
        "      msa, pair = blk(msa, pair)\n",
        "    # return logits for each msa position\n",
        "    return self.msa_out(msa), pair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainConfig:\n",
        "  DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  LR           = 1e-4\n",
        "  WD           = 1e-4\n",
        "  AMS          = True\n",
        "  WARMUP       = 50\n",
        "  EPOCHS       = 500\n",
        "  BATCH        = 8\n",
        "  MSA_SEQ      = 32       # number of sequences in each MSA\n",
        "  L_SEQ        = 256      # length of each sequence\n",
        "  EVAL_ITERS   = 5\n",
        "  EVAL_INTV    = 50"
      ],
      "metadata": {
        "id": "tijjqXJWaOE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "msa_data  = np.load(\"msa.npy\")   # shape (D, N, L, A)\n",
        "pair_data = np.load(\"pair.npy\")  # shape (D, L, L, C)\n",
        "assert msa_data.ndim==4 and pair_data.ndim==4"
      ],
      "metadata": {
        "id": "6tP5VdDQae5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ 3. Train/Val Split ------\n",
        "D = msa_data.shape[0]\n",
        "split = int(D * 0.85)\n",
        "msa_train, msa_val   = msa_data[:split], msa_data[split:]\n",
        "pair_train, pair_val = pair_data[:split], pair_data[split:]"
      ],
      "metadata": {
        "id": "tEbkhvPZacaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ 4. Model, Optimizer, Scheduler ------\n",
        "model = Evoformer(ModelConfig).to(ModelConfig.DEVICE)\n",
        "n_param = sum(p.numel() for p in _model.parameters())/1e6\n",
        "print(f\"{n_param:.2f} million\")\n",
        "\n",
        "opt   = AdamW(model.parameters(), lr=TrainConfig.LR, weight_decay=TrainConfig.WD, amsgrad=TrainConfig.AMS)\n",
        "warm  = LambdaLR(opt, lambda e: min((e+1)/TrainConfig.WARMUP, 1.0))\n",
        "cos   = CosineAnnealingLR(opt, T_max=TrainConfig.EPOCHS-TrainConfig.WARMUP, eta_min=1e-6)"
      ],
      "metadata": {
        "id": "ItPQEq0IaabW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
        "\n",
        "# ------ 5. Batch Sampler ------\n",
        "def get_batch(split):\n",
        "  if split==\"train\":\n",
        "    msa, pair = msa_train, pair_train\n",
        "  else:\n",
        "    msa, pair = msa_val,   pair_val\n",
        "  idx = np.random.randint(0, msa.shape[0], size=TrainConfig.BATCH)\n",
        "  # each batch: (B, N, L, A) and (B, L, L, C)\n",
        "  return (\n",
        "    torch.tensor(msa[idx],  dtype=torch.float32, device=TrainConfig.DEVICE),\n",
        "    torch.tensor(pair[idx], dtype=torch.float32, device=TrainConfig.DEVICE)\n",
        "  )\n",
        "\n",
        "# ------ 6. Eval Loss (masked‑token CE) ------\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  model.eval()\n",
        "  out = {}\n",
        "  for split in (\"train\",\"val\"):\n",
        "    losses = []\n",
        "    for _ in range(TrainConfig.EVAL_ITERS):\n",
        "      M, P = get_batch(split)\n",
        "      logits, _ = model(M, P)\n",
        "      # masked‑token: randomly mask 15% of msa positions\n",
        "      mask = (torch.rand_like(logits[...,0]) < 0.15)\n",
        "      target = M.argmax(-1)  # (B,N,L)\n",
        "      logits = logits[mask]\n",
        "      target = target[mask]\n",
        "      losses.append(F.cross_entropy(logits, target).item())\n",
        "    out[split] = sum(losses)/len(losses)\n",
        "  model.train()\n",
        "  return out\n",
        "\n",
        "# ------ 7. Training Loop ------\n",
        "history = []\n",
        "for epoch in range(TrainConfig.EPOCHS):\n",
        "  M, P = get_batch(\"train\")\n",
        "  opt.zero_grad()\n",
        "  logits, _ = model(M, P)\n",
        "  mask   = (torch.rand_like(logits[...,0]) < 0.15)\n",
        "  target = M.argmax(-1)\n",
        "  loss   = F.cross_entropy(logits[mask], target[mask])\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  opt.step()\n",
        "  if epoch < TrainConfig.WARMUP: warm.step()\n",
        "  else:            cos.step()\n",
        "\n",
        "  if (epoch+1)%TrainConfig.EVAL_INTV==0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Epoch {epoch+1:4d} | train {losses['train']:.4f}  val {losses['val']:.4f}\")\n",
        "    history.append((epoch+1, losses['train'], losses['val']))"
      ],
      "metadata": {
        "id": "HCxCTj5DaLM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ 8. Save & Plot ------\n",
        "n_param = sum(p.numel() for p in _model.parameters())/1e6\n",
        "model_save_name = f'evo_{n_param:.0f}m.pth'\n",
        "path = f\"/content/drive/MyDrive/{model_save_name}\"\n",
        "torch.save(_model.state_dict(), path)"
      ],
      "metadata": {
        "id": "fHxwti5zaktU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import matplotlib.pyplot as plt\n",
        "  e,t,v = zip(*history)\n",
        "  plt.plot(e,t,label=\"train\"); plt.plot(e,v,label=\"val\")\n",
        "  plt.legend(); plt.show()\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "uHfRPbTVamHZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}