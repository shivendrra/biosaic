{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendrra/biosaic/blob/main/training%20files/biosaic_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8l3EoGdNFIf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtFDt32DJOR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ybtg7t1iJXNP"
      },
      "outputs": [],
      "source": [
        "class ModelConfig:\n",
        "  d_model: int= 256\n",
        "  in_dim: int= 4\n",
        "  n_embed: int= 256\n",
        "  beta: float= 0.1\n",
        "  dropout: float= 0.2\n",
        "  n_heads: int= 8\n",
        "  n_layers: int= 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygZ9NnY2JapL"
      },
      "outputs": [],
      "source": [
        "class encoder(nn.Module):\n",
        "  def __init__(self, _in, d_model, n_layers, n_heads):\n",
        "    super().__init__()\n",
        "    self.embed = nn.Linear(_in, d_model)\n",
        "    self.encoder = nn.TransformerEncoder(\n",
        "      nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads),\n",
        "      num_layers=n_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embed(x)\n",
        "    x = x.permute(1, 0, 2)  # (L, B, d_model)\n",
        "    z_e = self.encoder(x) # Transformer encoding\n",
        "    return z_e.permute(1, 0, 2) # Back to (B, L, 4)\n",
        "\n",
        "class decoder(nn.Module):\n",
        "  def __init__(self, d_model, _out, n_layers, n_heads):\n",
        "    super().__init__()\n",
        "    self.decoder = nn.TransformerDecoder(\n",
        "      nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads),\n",
        "      num_layers=n_layers\n",
        "    )\n",
        "    self.fc_out = nn.Linear(d_model, _out)  # Output logits (4 classes)\n",
        "\n",
        "  def forward(self, z_q):\n",
        "    z_q = z_q.permute(1, 0, 2)  # (L, B, d_model)\n",
        "    x_recon = self.decoder(z_q, z_q)  # Transformer decoding\n",
        "    x_recon = self.fc_out(x_recon.permute(1, 0, 2))  # Back to (B, L, 4)\n",
        "    return x_recon\n",
        "\n",
        "class Quantizer(nn.Module):\n",
        "  def __init__(self, n_embed, d_model, beta):\n",
        "    super().__init__()\n",
        "    self.n_embed, self.d_model, self.beta = n_embed, d_model, beta\n",
        "    self.embeddings = nn.Embedding(n_embed, d_model)\n",
        "    self.embeddings.weight.data.uniform_(-1.0 / n_embed, 1.0 / n_embed)\n",
        "\n",
        "  def forward(self, z_e):\n",
        "    z_e_flat = z_e.reshape(-1, self.d_model)\n",
        "    distances = torch.cdist(z_e_flat, self.embeddings.weight)\n",
        "    encoding_indices = torch.argmin(distances, dim=1)\n",
        "    z_q = self.embeddings(encoding_indices).view(z_e.shape)\n",
        "    loss = self.beta * torch.mean((z_q.detach() - z_e) ** 2) + torch.mean((z_e.detach() - z_q) ** 2)\n",
        "\n",
        "    z_q = z_e + (z_q - z_e).detach()\n",
        "    return z_q, loss, encoding_indices.view(z_e.shape[:-1])\n",
        "\n",
        "class DNA_VQVAE(nn.Module):\n",
        "  def __init__(self, args: ModelConfig):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder(args.in_dim, args.d_model, args.n_layers, args.n_heads)\n",
        "    self.vq_layer = Quantizer(args.n_embed, args.d_model, args.beta)\n",
        "    self.decoder = decoder(args.d_model, args.in_dim, args.n_layers, args.n_heads)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z_e = self.encoder(x)\n",
        "    z_q, vq_loss, indices = self.vq_layer(z_e)\n",
        "    x_recon = self.decoder(z_q)\n",
        "    return x_recon, vq_loss, indices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DNA_VOCAB = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "INDEX_TO_DNA = {v: k for k, v in DNA_VOCAB.items()}  # Reverse mapping\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, path:str):\n",
        "    self.path = path\n",
        "\n",
        "  def load_simple(self):\n",
        "    with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
        "      lines = [line.strip() for line in f.readlines() if line.strip()]  # removing empty lines & strip whitespace\n",
        "    merged_sequence = \"\".join(lines)  # joining all lines into a single sequence\n",
        "    return merged_sequence\n",
        "\n",
        "  def load_encoded(self, seq=None):\n",
        "    if seq:\n",
        "      loaded_sequences = seq\n",
        "    else:\n",
        "      loaded_sequences = self.load_simple()\n",
        "    seq_idx = [DNA_VOCAB[char] for char in loaded_sequences]\n",
        "    return F.one_hot(torch.tensor(seq_idx, dtype=torch.long), num_classes=4) # shape (L, 4)\n",
        "\n",
        "  def train_test_split(self, sequence:str=None, ratio:float=0.8):\n",
        "    sequence = self.load_encoded(seq=sequence) if sequence else self.load_encoded()\n",
        "    split_size = int(0.8 * len(sequence))\n",
        "\n",
        "    train_data = sequence[:split_size]\n",
        "    test_data = sequence[split_size:]\n",
        "    return train_data, test_data"
      ],
      "metadata": {
        "id": "n87scqoagYth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "file_path = \"/content/drive/MyDrive/dna_data2.txt\"\n",
        "data = Dataset(file_path)\n",
        "train_data, val_data = data.train_test_split(ratio=0.75)"
      ],
      "metadata": {
        "id": "tINtocBxcP5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train params\n",
        "device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "learning_rate = 1e-4         # bumped from 1e-5\n",
        "weight_decay  = 1e-4\n",
        "amsgrad       = True\n",
        "warmup_epochs = 50           # linear warm‑up\n",
        "epochs        = 2000\n",
        "eval_interval = 100\n",
        "eval_iters    = 30\n",
        "batch_size    = 6\n",
        "block_size    = 256\n",
        "loss_history  = []"
      ],
      "metadata": {
        "id": "1c_B7Nd1YPq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVPlj5JLJsn-"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
        "\n",
        "_model = DNA_VQVAE(ModelConfig).to(\"cpu\")\n",
        "n_param = sum(p.numel() for p in _model.parameters())/1e6\n",
        "print(f\"{n_param:.2f} million\")\n",
        "optimizer = torch.optim.Adam(_model.parameters(), lr=learning_rate, amsgrad=True, weight_decay=1e-5, betas=(0.9, 0.95))\n",
        "\n",
        "# ——— Learning‑rate Schedulers ———\n",
        "# 1) Warm‑up: linearly ramp LR from 0 → lr over warmup_epochs\n",
        "warmup_scheduler = LambdaLR(\n",
        "  optimizer,\n",
        "  lr_lambda=lambda epoch: min((epoch+1)/warmup_epochs, 1.0)\n",
        ")\n",
        "# 2) After warm‑up, cosine decay from lr → 0 over remaining epochs\n",
        "cosine_scheduler = CosineAnnealingLR(\n",
        "  optimizer,\n",
        "  T_max=epochs - warmup_epochs,\n",
        "  eta_min=1e-6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/biosaic_30m.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "_model.load_state_dict(checkpoint)\n",
        "_model = _model.to(\"cpu\")"
      ],
      "metadata": {
        "id": "fpY_t856YpIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]).float()  # Convert to float\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]).float()  # Convert to float\n",
        "  return x.to(\"cpu\"), y.to(\"cpu\")\n",
        "\n",
        "torch.manual_seed(1800)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  _model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      x_recon, vq_loss, _ = _model(X)\n",
        "      recon_loss = F.cross_entropy(x_recon.view(-1, 4), Y.view(-1, 4))\n",
        "      losses[k] = (recon_loss + vq_loss).item()\n",
        "    out[split] = losses.mean()\n",
        "  _model.train()\n",
        "  return out\n",
        "\n",
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "for epoch in range(epochs):\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  x_recon, vq_loss, _ = _model(xb)\n",
        "  recon_ce  = F.cross_entropy(x_recon.view(-1,4), yb.view(-1,4))\n",
        "  recon_mse = F.mse_loss(torch.softmax(x_recon, dim=-1), yb)\n",
        "  recon_loss = recon_ce + 0.5*recon_mse\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  recon_loss.backward()\n",
        "  # — Gradient clipping —\n",
        "  torch.nn.utils.clip_grad_norm_(_model.parameters(), max_norm=1.0)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # — Scheduler step —\n",
        "  if epoch < warmup_epochs:\n",
        "    warmup_scheduler.step()\n",
        "  else:\n",
        "    cosine_scheduler.step()\n",
        "\n",
        "  # — Logging & eval —\n",
        "  if (epoch+1) % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Epoch {epoch+1:4d} | train {losses['train']:.4f}  val {losses['val']:.4f}\")\n",
        "    loss_history.append((epoch+1, losses['train'], losses['val']))\n",
        "\n",
        "end_time = timeit.default_timer()\n",
        "print(f\"Total time taken: {(end_time - start_time) / 3600} hrs\")"
      ],
      "metadata": {
        "id": "gmZwOkeXfgtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_param = sum(p.numel() for p in _model.parameters())/1e6\n",
        "model_save_name = f'biosaic_{n_param:.0f}m.pth'\n",
        "path = f\"/content/drive/MyDrive/{model_save_name}\"\n",
        "torch.save(_model.state_dict(), path)"
      ],
      "metadata": {
        "id": "DgWHzuacZ4Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmniaeYvNeOR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_logged, train_losses, val_losses = zip(*loss_history)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epochs_logged, train_losses, label=\"Train Loss\", marker='o', linestyle='-')\n",
        "plt.plot(epochs_logged, val_losses, label=\"Val Loss\", marker='o', linestyle='--')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training & Validation Loss Over Time\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving safe-tensors\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "model_save_name = f'consolidated_00.safetensors'\n",
        "path = f\"/content/drive/MyDrive/{model_save_name}\"\n",
        "save_file(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "YnvlOP-laMtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUjT8LyQJf5L"
      },
      "outputs": [],
      "source": [
        "DNA_VOCAB = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3}\n",
        "INDEX_TO_DNA = {0: 'A', 1: 'C', 2: 'G', 3: 'T'}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DNA_VQVAE(ModelConfig).to(device)\n",
        "\n",
        "class tokenizer:\n",
        "  def __init__(self):\n",
        "    self.vocab = DNA_VOCAB\n",
        "    self.ids_to_dna = INDEX_TO_DNA\n",
        "    self.device = device\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"\\t/Biosaic VQ-VAE tokenizer v1.0.0/\\t\"\n",
        "\n",
        "  def dna_to_onehot(self, seq):\n",
        "    seq_idx = [DNA_VOCAB[char] for char in seq]\n",
        "    one_hot = F.one_hot(torch.tensor(seq_idx), num_classes=4)\n",
        "    return one_hot.float()\n",
        "\n",
        "  def onehot_to_dna(self, logits):\n",
        "    decoded_out = torch.argmax(logits, dim=-1)\n",
        "    decoded = ''.join(self.ids_to_dna[idx.item()] for idx in decoded_out.squeeze(0))\n",
        "    return decoded\n",
        "\n",
        "  def encode(self, seq: str):\n",
        "    one_hot_seq = self.dna_to_onehot(seq).unsqueeze(0).to(device)\n",
        "    _, _, tokens = model(one_hot_seq)\n",
        "    return tokens.cpu().numpy()\n",
        "\n",
        "  def decode(self, tokens: list):\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long).to(device)\n",
        "    z_q = model.vq_layer.embeddings(tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model.decoder(z_q)\n",
        "    decoded = self.onehot_to_dna(logits)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgQo9-IuI41Q"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/dna_data2.txt', 'r', encoding='utf-8') as file:\n",
        "  dataset = file.readlines()\n",
        "  dataset = \"\".join(line.strip() for line in dataset if line.strip())\n",
        "  dataset = dataset.upper()\n",
        "print(len(dataset)/1e6, 'million words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq = \"\"\"ACTACGCCACGCCAGTTGAACTGGTGCCGCTGTTAGAGGAAAAATCTTCATGGATGAGCCATGCCGCGCTGGTGTTTGGTCGCGAAGATTCCGGGTTGACTAACGAAGAGTTAGCGTTGGCTGACGTTCTTACTGGTGTGCCGATGGTGGCGGATTATCCTTCGCTCAATCTGGGGCAGGCGGTGATGGTCTATTGCTATCAATTAGCAACATTAATACAACAACCGGCGAAAAGTGATGCAACGGCAGACCAACATCAACTGCAAGCTTTACGCGAACGAGCCATGACATTGCTGACGACTCTGGCAGTGGCAGATGACATAAAACTGGTCGACTGGTTACAACAACGCCTGGGGCTTTTAGAGCAACGAGACACGGCAATGTTGCACCGTTTGCTGCATGATATTGAAAAAAATATCACCAAATAAAAAACGCCTTAGTAAGTATTTTTC\"\"\"\n",
        "token = tokenizer()\n",
        "encoded = token.encode(seq)\n",
        "decoded = token.decode(encoded)\n",
        "print(encoded)\n",
        "print(decoded)\n",
        "print(seq == decoded)"
      ],
      "metadata": {
        "id": "_LsPo4LQRuuV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuw6RbwaN+cyo4Gq8gOYcb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}